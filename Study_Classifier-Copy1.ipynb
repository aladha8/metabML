{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import logistic\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from statsmodels.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('study_19_processed.csv', index_col=0, dtype={'Disease': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_data = data.T[['npeaks','pcgroup','drt']].T.drop('Disease', axis=1)\n",
    "data_d = data.T.drop(labels=['npeaks','pcgroup', 'drt'], axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X,y) = (data_d.drop('Disease', axis=1), data_d['Disease'])\n",
    "dummies=pd.get_dummies(y)\n",
    "y = dummies.iloc[:,0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_scaler = StandardScaler()\n",
    "feature_data_scaled = feature_scaler.fit_transform(feature_data.T[['npeaks','drt']])\n",
    "feature_data.T[['npeaks', 'drt']] = feature_data_scaled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alimladha/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_data, npeaks=0, drt=0, group=False, log_scale=False):\n",
    "        self.npeaks = npeaks\n",
    "        self.drt = drt\n",
    "        self.group = group\n",
    "        self.feature_data = feature_data\n",
    "        self.log_scale = log_scale\n",
    "        \n",
    "    def fit(self, x, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, data):\n",
    "        if self.log_scale:\n",
    "            data = data.fillna(1).replace(0,1).apply(np.log10)\n",
    "        else:\n",
    "            data = data.fillna(0)\n",
    "        data = data.astype(float)\n",
    "        data = pd.concat([data,feature_data], axis=0)\n",
    "        data = data.T\n",
    "        index_to_drop=[]\n",
    "        for index,row in data.iterrows():\n",
    "            npeaks_data = row['npeaks']\n",
    "            drt_data = row['drt']\n",
    "            if npeaks_data<self.npeaks or drt_data<self.drt:\n",
    "                index_to_drop.append(index)\n",
    "        data = data.drop(index_to_drop).drop(['npeaks','drt'], axis=1)\n",
    "        if self.group:\n",
    "            data = data.astype(float).groupby('pcgroup').mean().T.values\n",
    "        else:\n",
    "            data = data.astype(float).drop('pcgroup', axis=1).T.values\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_selector = FeatureSelector(feature_data=feature_data, npeaks = 0, drt = 0, group=False, log_scale=True)\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=None)\n",
    "estimators = [('select_features',feature_selector),('scale',scaler), ('reduce_dim',pca)]\n",
    "pipe = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_fit) = pipe.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_fit).to_csv('X_fit.csv')\n",
    "pd.DataFrame(y_train).to_csv('y_fit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
